---
title: "Lecture 12"
subtitle: " "
author: "JMG"
institute: "MATH 204"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(faraway)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
library(broom)
mariokart <- mariokart %>% filter(total_pr <= 85)
```



# Multiple Regression

- Multiple regression builds on the foundations of simple linear regression to allow for more than one predictor. Watch the following video to get started.

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/sQpAuyfEYZg") %>%
  vembedr::use_align("center")
```

---

# Learning Objectives

- After this lecture, you should

--

  - know how to fit a multiple regression model using `lm`,
  
--

  - understand and be able to interpret adjusted $R^2$, and 
  
--

  - be able to use diagnostic plots to assess the validity of a linear fit. 
  
---

# Motivating Data

- Consider the `mariokart` data set which consists of auction data from Ebay for the game Mario Kart for the Nintendo Wii. This data was collected in early October 2009.

--

```{r}
head(mariokart)
```

--

- Let's obtain another view of this data.

---

# Glimpse of `mariokart`

```{r}
glimpse(mariokart)
```

--

- **Question:** What features affect the final price (`total_pr`) at which a game is sold? 

---

# A First Model

- As a start, we fit a linear model with the game condition (`cond`) as the only predictor:

--

```{r}
lm_fit <- lm(total_pr~cond,data=mariokart)
summary(lm_fit)
```

---

# Results

- Our first fit predicts that a used game will, on average, go for $10.90 less than a new game will. 

--

```{r,echo=FALSE,fig.height=4,warning=FALSE,message=FALSE}
lm_fit %>% 
  augment() %>% 
  ggplot() + 
  geom_point(aes(x=cond,y=total_pr),alpha=0.5) +
  geom_abline(slope=-6.623,intercept=53.771,color="blue",linewidth=1)
```


--

- **Question:** Do you think that the condition of the game alone is sufficient to predict the price of the game? Explain why or why not. 

---

# Adding Predictors

- As we will see, in R it is extremely easy to fit a model with many predictors. Why might we want to do this? 

--

- We would like to fit a model that includes all potentially important variables simultaneously. 

--


- Multiple regression can help us evaluate the relationship between a predictor variable and the outcome while controlling for the potential influence of other variables. 

--

- Let's fit a more complicated linear model.

---

# Multiple Regression Model

- A multiple regression model is a linear model with many predictors. In general, we write the model as

$$\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{k}x_{k}$$

when there are $k$ predictors. We always estimate the $\beta_{i}$ parameters using statistical software. 

--

- For example, we may want to use `cond`, `stock_photo` (whether the auction feature photo was a stock photo or not), `duration` (auction length, in days), and `wheels` (number of Wii wheels included in the auction) all as predictors of price for the `mariokart` data. 

--

- Let's obtain a linear fit with these predictors using `lm`. 

---

# Another Fit

```{r}
lm_fit2 <- lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart)
summary(lm_fit2)
```


---

# Results

- Notice that when we have controlled for other features, the condition (new versus used) of the game  has a smaller impact on the price of the game since the slope estimate has gone from -10.90 to -5.13. 

--

- For simple linear regression, we used $R^2$ to determine the amount of variability in the response that was explained by the model. Recall that

$$R^2 = 1 - \frac{\text{variability in residuals}}{\text{variability in the response}}$$

--

- $R^2$ does not work well for mulitple regression. Instead, we use **adjusted** $R^2$.

---

# Adjusted $R^2$

- The adjusted $R^2$ is computed as

$$R^2_{\text{adj}} = 1 - \frac{s_{\text{residuals}}^{2}}{s_{\text{response}}^{2}}\frac{n-1}{n-k-1}$$
where $n$ is the number of observations and $k$ is the number of predictor variables. Remember that a categorical predictor with $p$ levels will contribute $p-1$ to the number of variables in the model. 

--

- Notice that the adjusted $R^2$ will be smaller than the unadjusted $R^2$. 

--

- One of the main benefits of using adjusted $R^2$ for multiple regression is that it accounts for **model complexity**. 

--

- The best model is not always the most complicated one. For one, more complex models are more likely to overfit. 

---

# Model Selection 

- Model selection seeks to identify variables in the model that may not be helpful. 

--

- The model that includes all available explanatory variables is referred to as the full model. 

--

- There are a variety of model selection strategies that are used in practice. We will discuss two of the more common approaches. 


---

# Model Selecion Video

- This video provides further perspective on model selection. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/VB1qSwoF-l0") %>%
  vembedr::use_align("center")
```



---

# Model Selection Strategies

- **Backward Elimination**. In this approach, we would identify the predictor corresponding to the largest $p$-value. If the $p$-value is above the significance level (usually $\alpha=0.05$), then we drop that variable, refit the model, and repeat the process. If the largest $p$-value is less than the significance level, then we would not eliminate any predictors. 

--

- **Forward Selection**. This approach begins with no predictors, then we fit a model with each individual predictor one at a time and keep the predictor that has the smallest $p$-value. Forward selection proceeds by continuing to add at each step a predictor that results in the smallest $p$-value that is less than the significance level. When none of the remaining predictors can be added to the model and have a $p$-value less than the significance level, we stop. 

--

- It is important to note that backward elimination and forward selection may not produce the same final model.

---

# Model Selection Example

- Let's see both backward elimination and forward selection applied to the `mariokart` data. Note that the full model is (in R formula notation) `total_pr ~ cond + stock_photo + duration + wheels`.


--

- Let's work out the details together in R. 

--

- Backward elimination and forward selection use $p$-values in deciding which variables will make up the final model. However, there are other measures that are used in other approaches to model selection. For example, one could seek a model that has the largest adjusted $R^2$ value. Information theoretic measures such as [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) are also often used. A discussion on these matters falls outside the scope of this course.  

--

- We note that there are packages associated with statistical software that implement various variable selection algorithms. For example, [`olsrr`](https://olsrr.rsquaredacademy.com/) is an R package that implements a variety of variable selection methods. 


---

# Checking Model Conditions

- Multiple regression methods using the model 

$$\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{k}x_{k}$$
generally depend on the following four conditions:

--

  - the residuals of the model are nearly normal,
  
--

  - the variability of the residuals is nearly constant,
  
--

  - the residuals are independent, and
  
--

  - each variable is linearly related to the response. 
  
--

- Diagnostic plots can be used to check each of these conditions. 

---

# Histogram of Residuals

- A histogram of the residuals can be used to check for outliers. For example, the residuals for the final model for the `mariokart` data have the following histogram:

```{r,warning=FALSE,message=FALSE,echo=FALSE,fig.height=5}

lm_fit2 %>% 
  augment() %>% 
  ggplot(aes(x=.resid)) + geom_histogram(binwidth=2.5,color="black")
```

--

- There are no extreme outliers present. 

---

# Absolute Value of Residuals

- A plot of the absolute value of residuals versus fitted values is helpful to check the condition that the variance of residuals is approximately constant. 

--

```{r,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE}

lm_fit2 %>% 
  augment() %>%
  ggplot(aes(x=.fitted,y=abs(.resid))) + 
  geom_point() + geom_smooth(method="lm",se=FALSE,linetype="dashed")

```

--

- There is no evident distinguished pattern. 

---

# Additional Diagnostic Plots

- It can also be useful to examine the following types of plots:

--

  - Residuals in the order of their data collection. Such a plot is helpful in identifying any connection between cases that are close to one another. 
  
--

  - Residuals against each predictor. We are looking for any notable change in variability between groups. 
  
--

- These plots are shown for model results for the `mariokart` data on pages 369 and 370 of the textbook. Let's look at these together and discuss.  

---

# Diagnostic Plots Video

- Watch the following video for further perspective on assessing multiple regression with plots. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/3KSUeYMKt5A") %>%
  vembedr::use_align("center")
```


---

# Further Regression

- When it comes to regression, we have only scratched the surface. There is more we could discuss regarding multiple regression, and there are also other types of regression.  The following video provides an introduction to logistic regression
```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/uYC2eLVSpI8") %>%
  vembedr::use_align("center")
```

--

- For even more on regression, we recommend the text [Linear Models with R](https://julianfaraway.github.io/faraway/LMR/) by Faraway. 
