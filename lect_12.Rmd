---
title: "Lecture 12"
subtitle: " "
author: "JMG"
institute: "MATH 204"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(faraway)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
library(broom)
```


# Multiple Regression

- Multiple regression builds on the foundations of simple linear regression to allow for more than one predictor. Watch the following video to get started.

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/sQpAuyfEYZg") %>%
  vembedr::use_align("center")
```

---

# Learning Objectives

- After this lecture, you should

--

  - know how to fit a multiple regression model using `lm`,
  
--

  - understand and be able to interpret adjusted $R^2$, and 
  
--

  - be able to use diagnostic plots to assess the validity of a linear fit. 
  
---

# Motivating Data

- Consider the `mariokart` data set which consists of auction data from Ebay for the game Mario Kart for the Nintendo Wii. This data was collected in early October 2009.

--

```{r}
head(mariokart)
```

--

- Let's obtain another view of this data.

---

# Glimpse of `mariokart`

```{r}
glimpse(mariokart)
```

--

- **Question:** What features affect the final price (`total_pr`) at which a game is sold? 

---

# A First Model

- As a start, we fit a linear model with the game condition (`cond`) as the only predictor:

--

```{r}
lm_fit <- lm(total_pr~cond,data=mariokart)
summary(lm_fit)
```

---

# Results

- Our first fit predicts that a used game will, on average, go for $6.62 less than a new game will. 

--

```{r,echo=FALSE,fig.height=4,warning=FALSE,message=FALSE}
lm_fit %>% 
  augment() %>% 
  ggplot() + 
  geom_point(aes(x=cond,y=total_pr),alpha=0.5) +
  geom_abline(slope=-6.623,intercept=53.771,color="blue",linewidth=1) + 
  ylim(c(25,80))
```


--

- **Question:** Do you think that the condition of the game alone is sufficient to predict the price of the game? Explain why or why not. 

---

# Adding Predictors

- As we will see, in R it is extremely easy to fit a model with many predictors. WHy might we want to do this? 

--

- We would like to fit a model that includes all potentially important variables simultaneously. 

--


- Multiple regression can help us evaluate the relationship between a predictor variable and the outcome while controlling for the potential influence of other variables. 

--

- Let's fit a more complicated linear model.

---

# Multiple Regression Model

- A multiple regression model is a linear model with many predictors. In general, we write the model as

$$\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{k}x_{k}$$

when there are $k$ predictors. We always estimate the $\beta_{i}$ parameters using statistical software. 

--

- For example, we may want to use `cond`, `stock_photo` (whether the auction feature photo was a stock photo or not), `duration` (auction length, in days), and `wheels` (number of Wii wheels included in the auction) all as predictors of price for the `mariokart` data. 

--

- Let's obtain a linear fit with these predictors using `lm`. 

---

# Another Fit

```{r}
lm_fit2 <- lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart)
summary(lm_fit2)
```


---

# Results

- Notice that when we have controlled for other features, the condition (new versus used) of the game  has a smaller impact on the price of the game since the slope estimate has gone from -6.6 to -2.6. 

--

- For simple linear regression, we used $R^2$ to determine the amount of variability in the response that was explained by the model. Recall that

$$R^2 = 1 - \frac{\text{variability in residuals}}{\text{variability in the response}}$$

--

- $R^2$ does not work well for mulitple regression. Instead, we use **adjusted** $R^2$.

---

# Adjusted $R^2$

- The adjusted $R^2$ is computed as

$$R^2_{\text{adj}} = 1 - \frac{s_{\text{residuals}}^{2}}{s_{\text{response}}^{2}}\frac{n-1}{n-k-1}$$
where $n$ is the number of observations and $k$ is the number of predictor variables. Remember that a categorical predictor with $p$ levels will contribute $p-1$ to the number of variables in the model. 

--

- Notice that the adjusted $R^2$ will by smaller than the unadjusted $R^2$. 

--

- One of the main benefits of using adjusted $R^2$ for multiple regression is that it accounts for **model complexity**. 

--

- The best model is not always the most complicated one. For one, more complex models are more likely to overfit. 

---

# Model Selection 

