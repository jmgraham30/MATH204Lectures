---
title: "Lecture 9"
subtitle: " "
author: "JMG"
institute: "MATH 204"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
```

# Further Inference

- Now that we have introduced the foundations of statistical inference, we study in detail some of the most common basic inferences for 

--

  - a single proportion (6.1)
  
--

  - a difference of two proportions (6.2)
  
--

  - one-sample means (7.1)
  
--

  - paired data (7.3)
  
--

  - a difference of two means (7.3)
  
--

- After we discuss these topics along with power calculations and simple ordinary least squares regression (7.4 and Chapter 8), we will return to discuss goodness of fit (6.3) and testing for independence (6.4).


---

# Learning Objectives

- After this lecture, you should be able to conduct and apply point estimates, interval estimates, and hypothesis tests for
  
--

  - proportions, difference of proportions, one sample means, paired data, and difference of two means.
  
--

- This includes knowing when to use which type of hypothesis test, and

--

- what is the appropriate R command(s) to use.  

--

- Many of our interval estimates and hypothesis tests will rely on a central limit theorem. We quickly review these.  
  

---

# Central Limit Theorems

- Recall our CLT for sample proportions:

> When observations are independent and sample size is sufficiently large, then the sampling distribution for the sample proportion $\hat{p}$ is approximately normal with mean $\mu_{\hat{p}}=p$ and standard error $SE_{\hat{p}}=\sqrt{\frac{p(1-p)}{n}}$.

--

- We also have a CLT for the sample mean:

> When we collect samples of sufficiently large size $n$ from a population with mean $\mu$ and standard deviation $\sigma$, then the sampling distribution for the sample mean $\bar{x}$ is approximately normal with mean $\mu_{\bar{x}} = \mu$ and standard error $SE_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$. 

--

- Later we will return to discuss precisely what we mean by "samples of sufficiently large size."

--

- We use sampling distributions for obtaining confidence intervals and conducting hypothesis tests. 


---

# Videos for CLT

- Central limit theorems are reviewed in the videos included on the next few slides.

---

# CLT Video 1

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/oLW_uzkPZGA") %>%
  vembedr::use_align("center")
```

---

# CLT Video 2

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/_iFAZgpWsx0") %>%
  vembedr::use_align("center")
```

---

# CLT Video 3

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/uVEj2uBJfq0") %>%
  vembedr::use_align("center")
```

---

# CI for a Proportion

- Once you've determined a one-proportion CI would be helpful for an application, there are four steps to constructing the interval:

--

  - Identify the point estimate $\hat{p}$ and the sample size $n$, and determine what confidence level you want.
  
--

  - Verify the conditions to ensure $\hat{p}$ is nearly normal, that is, check that $n\hat{p} \geq 10$ and $n(1-\hat{p}) \geq 10$. 
  
--

  - If the conditions hold, estimate the standard error $SE$ using $\hat{p}$, find the appropriate $z^{\ast}$, and compute $\hat{p} \pm z^{\ast} \times SE$ . 
  
--

 - Interpret the CI in the context of the problem. 


---

# Proportion CI Example

- Consider the following problem:

--

> Is parking a problem on campus? A randomly selected group of 89 faculty and staff are asked whether they are satisfied with campus parking or not. Of the 89 individuals surveyed, 23 indicated that they are satisfied. What is the proportion $p$ of faculty and staff that are satisfied with campus parking? 

--

- Explain why this problem is suitable for a one-proportion CI. Is this CLT applicable? If so, obtain a 95% CI for a point estimate.

---

# Proportion CI Video

- Confidence intervals for a proportion are reviewed in this video:

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/_iFAZgpWsx0") %>%
  vembedr::use_align("center")
```

---

# Hypothesis Test for a Proportion

- Once you've determined a one-proportion hypothesis test is the correct test for a problem, there are four steps to completing the test:

--

  - Identify the parameter of interest, list hypotheses, identify the significance level, and identify $\hat{p}$ and $n$. 
  
--

  - Verify conditions to ensure $\hat{p}$ is nearly normal under $H_{0}$. For one-proportion hypothesis tests, use the null value $p_{0}$ to check the conditions $np_{0} \geq 10$ and $n(1-p_{0}) \geq 10$. 
  
--

 - If the conditions hold, compute the standard error, again using $p_{0}$, compute the $Z$-score, and identify the p-value. 
 
--

  - Evaluate the hypothesis test by comparing the p-value to the significance level $\alpha$, and provide a conclusion in the context of the problem.  
  
---

# Hypotheses for One Proportion Tests

- The null hypothesis for a one-proportion test is typically stated as $H_{0}: p = p_{0}$ where $p_{0}$ is a hypothetical value for $p$. 

--

- The corresponding alternative hypothesis is then one of 

--

  - $H_{A}: p \neq p_{0}$ (two-sided), 
  
--

  - $H_{A}: p < p_{0}$ (one-sided less than), or
  
--

  - $H_{A}: p > p_{0}$ (one-sided greater than)  
  
---

# Proportion Hypothesis Test Example

> Is parking a problem on campus? A randomly selected group of 89 faculty and staff are asked whether they are satisfied with campus parking or not. Of the 89 individuals surveyed, 23 indicated that they are satisfied. 

--

- Let's study this problem using a hypothesis testing framework.  

--

- Here is relevant R code:

```{r}
p_hat <- 23/89
n <- 89
p0 <- 0.5
(c(n*p0,n*(1-p0))) # success-failure condition
SE <- sqrt((p0*(1-p0))/n)
(z_val <- (p_hat - p0)/SE)
(p_value <- 2*(pnorm(z_val)))
```

---

# R Command(s) for Proportion Hypothesis Test

- In cases where the CLT applies, we can use a normal distribution to directly compute p-values with the `pnorm` function. 

--

- Alternatively, R has a built-in function `prop.test` that automates one-proportion hypothesis testing. 

--

- So, in our parking problem example we would use 
```{r}
prop.test(23,89,p=0.5,correct = FALSE)
```


---

# Problems to Practice

- Let's take a minute to do some practice problems. 

--

- Try problems 6.7 and 6.13 from the textbook. 

---

# Problems Involving Difference of Proportions

- Consider the following problem:

> An online poll on January 10, 2017 reported that 97 out of 120 people in Virginia between the ages of 18 and 29 believe that marijuana should be legal, while 84 out of 111 who are 30 and over held this belief. Is there a difference between the proportion of young people who favor marijuana leagalization as compared to people who are older?

--

- Think about how this problem is different than problems for a single proportion. The basic point here is that there are two groups, and we want to compare proportions across the two groups.

--

-  Data for the problem stated above might look as follows (only the first few rows are shown).

```{r _weed_data,echo=FALSE}
support_legalize <- c(rep("Yes",97),rep("No",23),rep("Yes",84),rep("No",27))
age_group <- c(rep("18-29",120),rep(">30",111))
age_group <- factor(age_group,levels=c("18-29",">30"))
my_df <- tibble(support_legalize=support_legalize,age_group=age_group)
my_df %>% slice_sample(n=6)
```

---

# Plotting Grouped Proportion Data

- We can used grouped bar plots or a mosaic plot to visualize data related to grouped proportions:

```{r grouped_prop_plot,fig.height=5,fig.width=11,echo=FALSE}
p1 <- my_df %>% ggplot(aes(x=age_group,fill=support_legalize)) + geom_bar()
p2 <- my_df %>% ggplot() + geom_mosaic(aes(x = product(support_legalize,age_group), fill=support_legalize))

p1 + p2
```

---

# Sampling Distribution for Difference of Proportions

The difference $\hat{p}_{1}-\hat{p}_{2}$ can be modeled using a normal distribution when

  - The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. 
  
  - The success-failure condition holds for both groups, where we check successes and failures in each group separately. 
  
--

- When these conditions are satisfied, the standard error of $\hat{p}_{1}-\hat{p}_{2}$ is

$$SE = \sqrt{\frac{p_{1}(1-p_{1})}{n_{1}} + \frac{p_{2}(1-p_{2})}{n_{2}}},$$
where $p_{1}$ and $p_{2}$ represent the population proportions, and $n_{1}$ and $n_{2}$ represent the sample sizes. 

---

# CI for Difference of Proportions

- To construct a CI for a difference of proportion, we can apply the formulas

$$SE = \sqrt{\frac{p_{1}(1-p_{1})}{n_{1}} + \frac{p_{2}(1-p_{2})}{n_{2}}},\ \ CI = \text{point estimate} \pm z^{\ast} \times SE$$

--

- If necessary, we can use the plug-in principle to estimate the standard error. 

$$SE \approx \sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}} + \frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}}$$

---

# CI for Difference of Proportions Examples

- Let's obtain a **90%** CI for the difference of proportions in the marijuana legalization question. Here is the relevant R code:
```{r}
n1 <- 120
n2 <- 111
p1 <- 97/n1
p2 <- 30/n2
p_diff <- p1 - p2
(c(n1*p1,n1*(1-p1),n2*p2,n2*(1-p2))) # success-failure conditions
SE <- sqrt(((p1*(1-p1))/n1) + ((p2*(1-p2))/n2))
z_90 <- -qnorm(0.05) # find the appropriate z-value for 90% CI
(CI <- p_diff + z_90 * c(-1,1) * SE) # 90% CI
```

--

- Let's try some more examples together. 

---

# Hypotheses for Difference of Proportion Tests

- The null hypothesis for a difference of proportions test is typically stated as $H_{0}: p_{1} - p_{2} = p_{0}$ where $p_{0}$ is a hypothetical value for $p$. 

--

- The corresponding alternative hypothesis is then one of 

--

  - $H_{A}: p_{1} - p_{2} \neq p_{0}$ (two-sided), 
  
--

  - $H_{A}: p_{1} - p_{2} < p_{0}$ (one-sided less than), or
  
--

  - $H_{A}: p_{1} - p_{2} > p_{0}$ (one-sided greater than)  
  
--

- When $p_{0} = 0$ we need to use the **pooled proportion**.

---

# Pooled Proportion

- When the null hypothesis is that the proportions are equal, use the pooled proportion  $\hat{p}_{\text{pooled}}$, where

$$\hat{p}_{\text{pooled}} = \frac{\text{number of "successes"}}{\text{number of cases}}=\frac{\hat{p}_{1}n_{1} + \hat{p}_{2}n_{2}}{n_{1} + n_{2}}$$

--

- The pooled proportion  $\hat{p}_{\text{pooled}}$ is used to check the success-failure condition and to estimate the standard error. 

---

# Hypothesis Test for Difference of Proportions

- Let's apply the hypothesis testing framework to this problem:

> An online poll on January 10, 2017 reported that 97 out of 120 people in Virginia between the ages of 18 and 29 believe that marijuana should be legal, while 84 out of 111 who are 30 and over held this belief. Is there a difference between the proportion of young people who favor marijuana leagalization as compared to people who are older?

--

```{r}
n1 <- 120; n2 <- 111
p1 <- 97/120; p2 <- 84/111; p_diff <- p1 - p2
p_pooled <- (p1*n1+p2*n2)/(n1+n2)
(c(n*p_pooled,n*(1-p_pooled)))
SE <- sqrt((p_pooled*(1-p_pooled))/n1 + (p_pooled*(1-p_pooled))/n2)
(z_val <- (p_diff - 0.0)/SE)
(p_value <- 2*(1-pnorm(z_val)))
```


---

# R Commands for Difference of Proportion Test

- The R command `prop.test` also works for testing a difference of proportions. 

--

- For example, we could solve the marijuana legalization problem with the following:
```{r}
prop.test(c(97,84),c(120,111),correct = FALSE)
```

---

# Hypothesis Test for Difference of Proportions Examples

- Let's work some more problems together. 

---

# Inference for a Sample Mean

- A potato chip manufacturer claims that there is on average 32 chips per bag for their brand. How can we tell if this is an accurate claim? 

--

- One approach is to take a sample of, say 25 bags of chips for the particular band and compute the sample mean number of chips per bag. 

--

- This type of problem is inference for a sample mean. 

--

- In principle, we know the sampling distribution for the sample mean is very nearly normal, provided the conditions for the CLT holds.

--

- One question is, what are the appropriate conditions to check to make sure the CLT holds for the sample mean? 

--

- When the conditions for the CLT for the sample mean hold, the expression for the standard error involves the population standard deviation ($\sigma$) which we rarely know if practice. So, how do we estimate the standard error for the sample mean? 

--

- We address these questions in the next few slides. 

---

# Inference for a Mean Video


- You are encouraged to watch this video on inference for a mean.

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/RYVIGj1l4xs") %>%
  vembedr::use_align("center")
```


---

# Conditions for CLT for Sample Mean

- Two conditions are required to apply the CLT for a sample mean $\bar{x}$:

--

> **Independence:** The sample observations must be independent. Simple random samples from a population are independent, as are data from a random process like rolling a die or tossing a coin. 

--

> **Normality:** When a sample is small, we also require that the sample observations come from a normally distributed population. This condition can be relaxed for larger sample sizes. 

--

- The normality condition is vague but there are some approximate rules that work well in practice. 


---

# Practical Check for Normality 

- ** $n < 30$:** If the sample size $n$ is less than 30 and there are no clear outliers, then we can assume the data come from a nearly normal distribution. 

--

- ** $n \geq 30$:** If the sample size $n$ is at least 30 and there are no particularly exterme outliers, then we can assume the sampling distribution of $\bar{x}$ is nearly normal, even if the underlying distribution of individual observations is not. 

--

- Let's consider example 7.1 from the textbook to illustrate these practical rules. 

---

# Estimating Standard Error

- When samples are independent and the normality condition is met, then the sampling distribution for the sample mean $\bar{x}$ is (very nearly) normal with mean $\mu_{\bar{x}} = \mu$ and standard error $SE_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$, where $\mu$ is the true population mean of the distribution from which the samples are taken and $\sigma$ is the true population from which the samples are taken. 

--

- In practice we do not know the true values of the population parameters  $\mu$ and $\sigma$. But we can use the plug-in principle to obtain estimates:

$$\mu_{\bar{x}} \approx \bar{x}, \ \ \text{and} \ \ SE_{\bar{x}} \approx \frac{s}{\sqrt{n}},$$

where $s$ is the **sample standard deviation**. 

---

# T-score

- If we take $n$ samples from a normal distribution $N(\mu,\sigma)$, then the quantity

$$z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$$
will follow a standard normal distribution $N(0,1)$.

--

- On the other hand, the quantity

$$t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$$
will not quite be $N(0,1)$, especially if $n$ is small. 

--

- We call the quantity in the last formula a T-score.  

---

# T-Score Video

- Let's watch this video on T-scores. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/k5Ets4QJYmY") %>%
  vembedr::use_align("center")
```


---

# t-distribution

- T-scores follow a t-distribution with degrees of freedom $\text{df}=n-1$, where $n$ is the sample size. 

--

- We will use a t-distributions for inference, that is, for obtaining confidence intervals, and hypothesis testing. 

--

- Let's get a feel for t-distributions. 

---

# Plotting t-Distributions

- We can easily plot density curves for t-distributions.

```{r,fig.height=5}
gf_dist("t",df=19)
```

---

# Degrees of Freedom Parameter

- This plot shows three different t density functions for three different degrees of freedom:

```{r,fig.height=6,fig.width=10,echo=FALSE}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dt, args = list(df=2), aes(color='df=2'),lwd=1) +
  stat_function(fun = dt, args = list(df=5), aes(color='df=5'),lwd=1) + 
  stat_function(fun = dt, args = list(df=10), aes(color='df=10'),lwd=1) + 
  scale_color_manual(name="Degrees of Freedom",
                     values=c("df=2"="#88CCEE","df=5"="#CC6677","df=10"="#117733"))
```

---

# A Computational Experiment

- This plot shows a histogram of T-score values obtained by computing the sample mean with $n=8$ from $N(0,1)$. We overlay a density curve for both $N(0,1)$ and a t-distribution with $\text{df}=7$:

```{r,fig.height=5,echo=FALSE}
get_data <- function(n){
  x <- rnorm(8)
  x_bar <- mean(x)
  x_sd <- sd(x)
  t_score <- x_bar/(x_sd/sqrt(length(x)))
}

t_scores <- map_dbl(1:2500,get_data)

t_scores_df <- tibble(t_scores=t_scores)

t_scores_df %>% 
  ggplot(aes(x=t_scores)) + 
  geom_histogram(aes(y=..density..),color="black") + 
  stat_function(fun=dnorm,aes(color="Normal"),lwd=1) + 
  stat_function(fun=dt,args = list(df=7),aes(color="t"),lwd=1) + 
  scale_color_manual(name="Distribution",
                     values=c("Normal"="#88CCEE","t"="#CC6677"))
```

---

# Area Under a t-Distribution

- We can compute areas under t-distribution density curves in the same way we did for areas under normal distribution density curves. For example, the area

```{r,fig.height=3}
gf_dist("t",df=5,fill = ~(x<=-1.0),geom="area")
```


is computed by

```{r}
pt(-1.0,df=5)
```

---

# One-Sample t CI


---

# One-Sample t-test


---

# R Command for One-Sample t-test


---

# Paired Data

