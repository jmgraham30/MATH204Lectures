---
title: "Lecture 9"
subtitle: " "
author: "JMG"
institute: "MATH 204"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
```

# Further Inference

- Now that we have introduced the foundations of statistical inference, we study in detail some of the most common basic inferences for 

--

  - a single proportion (6.1)
  
--

  - a difference of two proportions (6.2)
  
--

  - one-sample means (7.1)
  
--

  - paired data (7.3)
  
--

  - a difference of two means (7.3)
  
--

- After we discuss these topics along with power calculations and simple ordinary least squares regression (7.4 and Chapter 8), we will return to discuss goodness of fit (6.3) and testing for independence (6.4).


---

# Learning Objectives

- After this lecture, you should be able to conduct and apply point estimates, interval estimates, and hypothesis tests for
  
--

  - proportions, difference of proportions, one sample means, paired data, and difference of two means.
  
--

- This includes knowing when to use which type of hypothesis test, and

--

- what is the appropriate R command(s) to use.  

--

- Many of our interval estimates and hypothesis tests will rely on a central limit theorem. We quickly review these.  
  

---

# Central Limit Theorems

- Recall our CLT for sample proportions:

> When observations are independent and sample size is sufficiently large, then the sampling distribution for the sample proportion $\hat{p}$ is approximately normal with mean $\mu_{\hat{p}}=p$ and standard error $SE_{\hat{p}}=\sqrt{\frac{p(1-p)}{n}}$.

--

- We also have a CLT for the sample mean:

> When we collect samples of sufficiently large size $n$ from a population with mean $\mu$ and standard deviation $\sigma$, then the sampling distribution for the sample mean $\bar{x}$ is approximately normal with mean $\mu_{\bar{x}} = \mu$ and standard error $SE_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$. 

--

- Later we will return to discuss precisely what we mean by "samples of sufficiently large size."

--

- We use sampling distributions for obtaining confidence intervals and conducting hypothesis tests. 


---

# Videos for CLT

- Central limit theorems are reviewed in the videos included on the next few slides.

---

# CLT Video 1

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/oLW_uzkPZGA") %>%
  vembedr::use_align("center")
```

---

# CLT Video 2

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/_iFAZgpWsx0") %>%
  vembedr::use_align("center")
```

---

# CLT Video 3

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/uVEj2uBJfq0") %>%
  vembedr::use_align("center")
```

---

# CI for a Proportion

- Once you've determined a one-proportion CI would be helpful for an application, there are four steps to constructing the interval:

--

  - Identify the point estimate $\hat{p}$ and the sample size $n$, and determine what confidence level you want.
  
--

  - Verify the conditions to ensure $\hat{p}$ is nearly normal, that is, check that $n\hat{p} \geq 10$ and $n(1-\hat{p}) \geq 10$. 
  
--

  - If the conditions hold, estimate the standard error $SE$ using $\hat{p}$, find the appropriate $z^{\ast}$, and compute $\hat{p} \pm z^{\ast} \times SE$ . 
  
--

 - Interpret the CI in the context of the problem. 


---

# Proportion CI Example

- Consider the following problem:

--

> Is parking a problem on campus? A randomly selected group of 89 faculty and staff are asked whether they are satisfied with campus parking or not. Of the 89 individuals surveyed, 23 indicated that they are satisfied. What is the proportion $p$ of faculty and staff that are satisfied with campus parking? 

--

- Explain why this problem is suitable for a one-proportion CI. Is this CLT applicable? If so, obtain a 95% CI for a point estimate.

---

# Proportion CI Video

- Confidence intervals for a proportion are reviewed in this video:

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/_iFAZgpWsx0") %>%
  vembedr::use_align("center")
```

---

# Hypothesis Test for a Proportion

- Once you've determined a one-proportion hypothesis test is the correct test for a problem, there are four steps to completing the test:

--

  - Identify the parameter of interest, list hypotheses, identify the significance level, and identify $\hat{p}$ and $n$. 
  
--

  - Verify conditions to ensure $\hat{p}$ is nearly normal under $H_{0}$. For one-proportion hypothesis tests, use the null value $p_{0}$ to check the conditions $np_{0} \geq 10$ and $n(1-p_{0}) \geq 10$. 
  
--

 - If the conditions hold, compute the standard error, again using $p_{0}$, compute the $Z$-score, and identify the p-value. 
 
--

  - Evaluate the hypothesis test by comparing the p-value to the significance level $\alpha$, and provide a conclusion in the context of the problem.  
  
---

# Hypotheses for One Proportion Tests

- The null hypothesis for a one-proportion test is typically stated as $H_{0}: p = p_{0}$ where $p_{0}$ is a hypothetical value for $p$. 

--

- The corresponding alternative hypothesis is then one of 

--

  - $H_{A}: p \neq p_{0}$ (two-sided), 
  
--

  - $H_{A}: p < p_{0}$ (one-sided less than), or
  
--

  - $H_{A}: p > p_{0}$ (one-sided greater than)  
  
---

# Proportion Hypothesis Test Example

> Is parking a problem on campus? A randomly selected group of 89 faculty and staff are asked whether they are satisfied with campus parking or not. Of the 89 individuals surveyed, 23 indicated that they are satisfied. 

--

- Let's study this problem using a hypothesis testing framework.  

--

- Here is relevant R code:

```{r}
p_hat <- 23/89
n <- 89
p0 <- 0.5
(c(n*p0,n*(1-p0))) # success-failure condition
SE <- sqrt((p0*(1-p0))/n)
(z_val <- (p_hat - p0)/SE)
(p_value <- 2*(pnorm(z_val)))
```

---

# R Command(s) for Proportion Hypothesis Test

- In cases where the CLT applies, we can use a normal distribution to directly compute p-values with the `pnorm` function. 

--

- Alternatively, R has a built-in function `prop.test` that automates one-proportion hypothesis testing. 

--

- So, in our parking problem example we would use 
```{r}
prop.test(23,89,p=0.5,correct = FALSE)
```


---

# Problems to Practice

- Let's take a minute to do some practice problems. 

--

- Try problems 6.7 and 6.13 from the textbook. 

---

# Problems Involving Difference of Proportions

- Consider the following problem:

> An online poll on January 10, 2017 reported that 97 out of 120 people in Virginia between the ages of 18 and 29 believe that marijuana should be legal, while 84 out of 111 who are 30 and over held this belief. Is there a difference between the proportion of young people who favor marijuana leagalization as compared to people who are older?

--

- Think about how this problem is different than problems for a single proportion. The basic point here is that there are two groups, and we want to compare proportions across the two groups.

--

-  Data for the problem stated above might look as follows (only the first few rows are shown).

```{r _weed_data,echo=FALSE}
support_legalize <- c(rep("Yes",97),rep("No",23),rep("Yes",84),rep("No",27))
age_group <- c(rep("18-29",120),rep(">30",111))
age_group <- factor(age_group,levels=c("18-29",">30"))
my_df <- tibble(support_legalize=support_legalize,age_group=age_group)
my_df %>% slice_sample(n=6)
```

---

# Plotting Grouped Proportion Data

- We can used grouped bar plots or a mosaic plot to visualize data related to grouped proportions:

```{r grouped_prop_plot,fig.height=5,fig.width=11,echo=FALSE}
p1 <- my_df %>% ggplot(aes(x=age_group,fill=support_legalize)) + geom_bar()
p2 <- my_df %>% ggplot() + geom_mosaic(aes(x = product(support_legalize,age_group), fill=support_legalize))

p1 + p2
```

---

# Sampling Distribution for Difference of Proportions

The difference $\hat{p}_{1}-\hat{p}_{2}$ can be modeled using a normal distribution when

  - The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. 
  
  - The success-failure condition holds for both groups, where we check successes and failures in each group separately. 
  
--

- When these conditions are satisfied, the standard error of $\hat{p}_{1}-\hat{p}_{2}$ is

$$SE = \sqrt{\frac{p_{1}(1-p_{1})}{n_{1}} + \frac{p_{2}(1-p_{2})}{n_{2}}},$$
where $p_{1}$ and $p_{2}$ represent the population proportions, and $n_{1}$ and $n_{2}$ represent the sample sizes. 

---

# CI for Difference of Proportions

- To construct a CI for a difference of proportion, we can apply the formulas

$$SE = \sqrt{\frac{p_{1}(1-p_{1})}{n_{1}} + \frac{p_{2}(1-p_{2})}{n_{2}}},\ \ CI = \text{point estimate} \pm z^{\ast} \times SE$$

--

- If necessary, we can use the plug-in principle to estimate the standard error. 

$$SE \approx \sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}} + \frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}}$$

---

# CI for Difference of Proportions Examples

- Let's obtain a **90%** CI for the difference of proportions in the marijuana legalization question. Here is the relevant R code:
```{r}
n1 <- 120
n2 <- 111
p1 <- 97/n1
p2 <- 84/n2
p_diff <- p1 - p2
(c(n1*p1,n1*(1-p1),n2*p2,n2*(1-p2))) # success-failure conditions
SE <- sqrt(((p1*(1-p1))/n1) + ((p2*(1-p2))/n2))
z_90 <- -qnorm(0.05) # find the appropriate z-value for 90% CI
(CI <- p_diff + z_90 * c(-1,1) * SE) # 90% CI
```

--

- Let's try some more examples together. 

---

# Hypotheses for Difference of Proportion Tests

- The null hypothesis for a difference of proportions test is typically stated as $H_{0}: p_{1} - p_{2} = p_{0}$ where $p_{0}$ is a hypothetical value for $p$. 

--

- The corresponding alternative hypothesis is then one of 

--

  - $H_{A}: p_{1} - p_{2} \neq p_{0}$ (two-sided), 
  
--

  - $H_{A}: p_{1} - p_{2} < p_{0}$ (one-sided less than), or
  
--

  - $H_{A}: p_{1} - p_{2} > p_{0}$ (one-sided greater than)  
  
--

- When $p_{0} = 0$ we need to use the **pooled proportion**.

---

# Pooled Proportion

- When the null hypothesis is that the proportions are equal, use the pooled proportion  $\hat{p}_{\text{pooled}}$, where

$$\hat{p}_{\text{pooled}} = \frac{\text{number of "successes"}}{\text{number of cases}}=\frac{\hat{p}_{1}n_{1} + \hat{p}_{2}n_{2}}{n_{1} + n_{2}}$$

--

- The pooled proportion  $\hat{p}_{\text{pooled}}$ is used to check the success-failure condition and to estimate the standard error. 

---

# Hypothesis Test for Difference of Proportions

- Let's apply the hypothesis testing framework to this problem:

> An online poll on January 10, 2017 reported that 97 out of 120 people in Virginia between the ages of 18 and 29 believe that marijuana should be legal, while 84 out of 111 who are 30 and over held this belief. Is there a difference between the proportion of young people who favor marijuana leagalization as compared to people who are older?

--

```{r}
n1 <- 120; n2 <- 111
p1 <- 97/120; p2 <- 84/111; p_diff <- p1 - p2
p_pooled <- (p1*n1+p2*n2)/(n1+n2)
(c(n*p_pooled,n*(1-p_pooled)))
SE <- sqrt((p_pooled*(1-p_pooled))/n1 + (p_pooled*(1-p_pooled))/n2)
(z_val <- (p_diff - 0.0)/SE)
(p_value <- 2*(1-pnorm(z_val)))
```


---

# R Commands for Difference of Proportion Test

- The R command `prop.test` also works for testing a difference of proportions. 

--

- For example, we could solve the marijuana legalization problem with the following:
```{r}
prop.test(c(97,84),c(120,111),correct = FALSE)
```

---

# Hypothesis Test for Difference of Proportions Examples

- Let's work some more problems together. 

---

# Inference for a Sample Mean

- A potato chip manufacturer claims that there is on average 32 chips per bag for their brand. How can we tell if this is an accurate claim? 

--

- One approach is to take a sample of, say 25 bags of chips for the particular band and compute the sample mean number of chips per bag. 

--

- This type of problem is inference for a sample mean. 

--

- In principle, we know the sampling distribution for the sample mean is very nearly normal, provided the conditions for the CLT holds.

--

- One question is, what are the appropriate conditions to check to make sure the CLT holds for the sample mean? 

--

- When the conditions for the CLT for the sample mean hold, the expression for the standard error involves the population standard deviation ( $\sigma$ ) which we rarely know if practice. So, how do we estimate the standard error for the sample mean? 

--

- We address these questions in the next few slides. 

---

# Inference for a Mean Video


- You are encouraged to watch this video on inference for a mean.

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/RYVIGj1l4xs") %>%
  vembedr::use_align("center")
```


---

# Conditions for CLT for Sample Mean

- Two conditions are required to apply the CLT for a sample mean $\bar{x}$:

--

> **Independence:** The sample observations must be independent. Simple random samples from a population are independent, as are data from a random process like rolling a die or tossing a coin. 

--

> **Normality:** When a sample is small, we also require that the sample observations come from a normally distributed population. This condition can be relaxed for larger sample sizes. 

--

- The normality condition is vague but there are some approximate rules that work well in practice. 


---

# Practical Check for Normality 

- ** $n < 30$:** If the sample size $n$ is less than 30 and there are no clear outliers, then we can assume the data come from a nearly normal distribution. 

--

- ** $n \geq 30$:** If the sample size $n$ is at least 30 and there are no particularly exterme outliers, then we can assume the sampling distribution of $\bar{x}$ is nearly normal, even if the underlying distribution of individual observations is not. 

--

- Let's consider example 7.1 from the textbook to illustrate these practical rules. 

---

# Estimating Standard Error

- When samples are independent and the normality condition is met, then the sampling distribution for the sample mean $\bar{x}$ is (very nearly) normal with mean $\mu_{\bar{x}} = \mu$ and standard error $SE_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$, where $\mu$ is the true population mean of the distribution from which the samples are taken and $\sigma$ is the true population from which the samples are taken. 

--

- In practice we do not know the true values of the population parameters  $\mu$ and $\sigma$. But we can use the plug-in principle to obtain estimates:

$$\mu_{\bar{x}} \approx \bar{x}, \ \ \text{and} \ \ SE_{\bar{x}} \approx \frac{s}{\sqrt{n}},$$

where $s$ is the **sample standard deviation**. 

---

# T-score

- If we take $n$ samples from a normal distribution $N(\mu,\sigma)$, then the quantity

$$z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$$
will follow a standard normal distribution $N(0,1)$.

--

- On the other hand, the quantity

$$t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$$
will not quite be $N(0,1)$, especially if $n$ is small. 

--

- We call the quantity in the last formula a T-score.  

---

# T-Score Video

- Let's watch this video on T-scores. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/k5Ets4QJYmY") %>%
  vembedr::use_align("center")
```


---

# t-distribution

- T-scores follow a t-distribution with degrees of freedom $\text{df}=n-1$, where $n$ is the sample size. 

--

- We will use t-distributions for inference, that is, for obtaining confidence intervals and hypothesis testing. 

--

- Let's get a feel for t-distributions. 

---

# Plotting t-Distributions

- We can easily plot density curves for t-distributions.

```{r,fig.height=5}
gf_dist("t",df=19)
```

---

# Degrees of Freedom Parameter

- This plot shows three different t density functions for three different degrees of freedom:

```{r,fig.height=6,fig.width=10,echo=FALSE}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dt, args = list(df=2), aes(color='df=2'),lwd=1) +
  stat_function(fun = dt, args = list(df=5), aes(color='df=5'),lwd=1) + 
  stat_function(fun = dt, args = list(df=10), aes(color='df=10'),lwd=1) + 
  scale_color_manual(name="Degrees of Freedom",
                     values=c("df=2"="#88CCEE","df=5"="#CC6677","df=10"="#117733"))
```

---

# A Computational Experiment

- This plot shows a histogram of T-score values obtained by computing the sample mean with $n=8$ from $N(0,1)$. We overlay a density curve for both $N(0,1)$ and a t-distribution with $\text{df}=7$:

```{r,fig.height=5,echo=FALSE,warning=FALSE,message=FALSE}
get_data <- function(n){
  x <- rnorm(8)
  x_bar <- mean(x)
  x_sd <- sd(x)
  t_score <- x_bar/(x_sd/sqrt(length(x)))
}

t_scores <- map_dbl(1:2500,get_data)

t_scores_df <- tibble(t_scores=t_scores)

t_scores_df %>% 
  ggplot(aes(x=t_scores)) + 
  geom_histogram(aes(y=..density..),color="black") + 
  stat_function(fun=dnorm,aes(color="Normal"),lwd=1) + 
  stat_function(fun=dt,args = list(df=7),aes(color="t"),lwd=1) + 
  scale_color_manual(name="Distribution",
                     values=c("Normal"="#88CCEE","t"="#CC6677"))
```

---

# Area Under a t-Distribution

- We can compute areas under t-distribution density curves in the same way we did for areas under normal distribution density curves. For example, the area

```{r,fig.height=3}
gf_dist("t",df=5,fill = ~(x<=-1.0),geom="area")
```


is computed by

```{r}
pt(-1.0,df=5)
```


---

# Middle 95% Under a t Density

- How do we find the middle 95% of area under a t-distribution? This is an important question because it relates to construting a 95% confidence interval for the sample mean. 

--

- Suppose we have a t-distribution with degrees of freedom $\text{df}=10$, then to find the value $t^{\ast}$ so that 95% of the area under the density curve lies between $-t^{\ast}$ and $t^{\ast}$, we use the `qt` command, for example, 

```{r}
(t_ast <- -qt(0.05/2,df=10)) 
```

--

- We can check our answer:

```{r}
pt(t_ast,df=10) - pt(-t_ast,df=10)
```

--

- Note that you always have to specify the appropriate value for the degrees of freedom df. The appropriate degrees of freedom is given by $\text{df}=n-1$, where $n$ is the sample size. 

---

# One-Sample t CI

> Based on a sample of $n$ independent and nearly normal observations, a confidence interval for the population mean is

$$\bar{x} \pm t^{\ast}_{\text{df}} \times \frac{s}{\sqrt{n}},$$

where $n$ is the sample size, $\bar{x}$ is the sample mean, $s$ is the sample standard deviation. 

--

- We determine the appropriate value for $t^{\ast}_{\text{df}}$ with the R command 

`qt((1.0-confidence_level)/2,df=n-1)`.



---

# One-Sample Mean CI Example

- Suppose we take a random sample of 13 observations from a normally distributed population and determine the sample mean is 8 with a sample standard deviation of 2.5. Then to find a 90% confidence interval, we would do the following:

```{r}
n <- 13
x_bar <- 8
s <- 2.5
SE <- s/sqrt(n)
t_ast <- -qt((1.0-0.9)/2,df=n-1)
(CI <- x_bar + t_ast * c(-1,1)*SE)
```

A 95% CI would be

```{r}
t_ast <- -qt((1.0-0.95)/2,df=n-1)
(CI <- x_bar + t_ast * c(-1,1)*SE)
```

---

# CI for Single Mean Summary

> Once you have determined a one-mean confidence interval would be helpful for an application, there are four steps to constructing the interval:

--

  - Identify $n$, $\bar{x}$, and $s$, and determine what confidence level you wish to use.
  
--

  - Verify the conditions to ensure $\bar{x}$ is nearly normal. 
  
--

  - If the conditions hold, approximate $SE$ by $\frac{s}{\sqrt{n}}$, find $t^{\ast}_{\text{df}}$, and construct the interval. 
  
--

  - Interpret the confidence interval in the context of the problem. 
  

---

# One-Mean Hypothesis Testing

- The null hypothesis for a one-proportion test is typically stated as $H_{0}: \mu = \mu_{0}$ where $\mu_{0}$ is the null value for the population mean. 

--

- The corresponding alternative hypothesis is then one of 

--

  - $H_{A}: \mu \neq \mu_{0}$ (two-sided), 
  
--

  - $H_{A}: \mu < \mu_{0}$ (one-sided less than), or
  
--

  - $H_{A}: \mu > \mu_{0}$ (one-sided greater than)  


---

# One-Mean Hypothesis Test Procedure

- Once you have determined a one-mean hypothesis test is the correct procedure, there are four steps to completing the test:

--

 - Identify the parameter of interest, list out hypotheses, identify the significance level, and identify $n$, $\bar{x}$, and $s$. 
 
--

 - Verify conditions to ensure $\bar{x}$ is nearly normal. 
 
--

 - If the conditions hold, approximate $SE$ by $\frac{s}{\sqrt{n}}$, compute the T-score using
 
 $$T = \frac{\bar{x} - \mu_{0}}{\frac{s}{\sqrt{n}}},$$
and compute the p-value. 

--

  - Evaluate the hypothesis test by comparing the p-value to the significance level $\alpha$, and provide a conclusion in the context of the problem. 
  
---

# A Simple Example

- We want to know if the following data comes from a normal distribution with mean 0:

```{r,echo=FALSE}
set.seed(123)
(x <- rnorm(10,0.2,1.0))
```

- We can apply a hypothesis test corresponding to $H_{0}: \mu = 0.0$, versus $H_{A}: \mu \neq 0$ as follows

```{r}
n <- length(x); x_bar <- mean(x); s <- sd(x)
alpha <- 0.05
SE <- s/sqrt(n)
mu_0 <- 0.0
(T <- (x_bar - mu_0)/SE)
(p_value <- 2*(1-pt(T,df=n-1))) 
```


---

# R Command for One-Sample t-test

- There is a built-in R command, `t.test` that will conduct a hypothesis test for a sinlg emean for us. 

--

- For example, we can solve our previous problem using

```{r}
t.test(x,mu=0.0)
```

--

- Let's work some examples together. 

---

# Paired Data

> Two sets of observations are *paired* if each observation in one set has a special correspondence or connection with exactly one observation in the other data set. 

--

- Common examples of paired data correspond to "before" and "after" trials. 

--

  - For example, does a particular study technique work well for increasing one's exam score? To test this, we can ask 25 people to take an exam and record their scores, then we can ask those same people to try the study technique before taking another similar exam. 
  
--

- As another example, suppose it is claimed that among the general population of adults in the US, the average length of the left foot is longer than the average length of the right foot. To test this, we can select 32 people, record the measurement of the left foot of everyone in one column, then record the measurement of the right foot of everyone in a second column. We must make sure that each row of the resulting data frame corresponds with only one person. 

---

# Paired Data Example

- Perhaps the data from our last example looks as follows:

```{r,echo=FALSE}
set.seed(123)
foot_df <- tibble(left_foot=rnorm(32,8,0.3),right_foot=rnorm(32,8,0.3)) %>% mutate(foot_diff=left_foot-right_foot)
foot_df %>% head()
```

--

- Notice that we have added a column that is the difference of the left foot measurement minus the right foot measurement. 

---

# Hypothesis Test for Paired Data

- How can we set up a hypothesis testing framework for the foot measurement question? 

--

- Basically, we can apply statistical inference to the difference column in the data. 

--

- The typical null hypothesis for paired data is that the average difference between the measurements is 0. We write this as

$$H_{0}: \mu_{d} = 0$$

---

# Paired Hypothesis Test Procedure

- Once you have determined a paired hypothesis test is the correct procedure, there are four steps to completing the test:

--

  - Determine the significance level, the sample size $n$, the mean of the differences $\bar{d}$, and the corresponding standard deviation $s_{d}$. 
 
--

  - Verify the conditions to ensure that $\bar{d}$ is nearly normal. 
  
--

  - If the conditions hold, approximate $SE$ by $\frac{s_{d}}{\sqrt{n}}$, compute the T-score using
  
  $$T = \frac{\bar{d}}{\frac{s_{d}}{\sqrt{n}}}$$
and compute the p-value. 

--

  - Evaluate the hypothesis test by comparing the p-value to the significance level $\alpha$, and provide a conclusion in the context of the problem. 
  
---

# First Example

- Consider our foot measurement data.The sample size is $n=32$ and a boxplot shows that there are no extreme outliers. Now we compute the necessary quantities:

```{r}
n <- 32
d_bar <- mean(foot_df$foot_diff)
s_d <- sd(foot_df$foot_diff)
(t_val <- d_bar/(s_d/sqrt(n)))
(p_val <- 2*pt(t_val,df=n-1))
```


--

- Here we fail to reject the null hypothesis at the 0.05 significance level. That is, the data does not provide sufficient evidence for rejecting the null hypothesis that there is no significant difference in the length of the left foot versus the right foot.


---

# R Command for Paired Hypothesis Test

- Again, we can use the `t.test` function. However, now we must use two sets of data and add the `paired=TRUE` argument. 

--

- For example, to test the hypothesis for the foot data, one would use

```{r}
t.test(foot_df$left_foot,foot_df$right_foot,paired = TRUE)
```

--

- Let's work some more examples together. 


---

# Difference of Two Means

- Inference for paired data compares two different (but related) measurements on the same population. For example, we might want to study the difference in how individuals sleep before and after consuming a large amount of caffeine. 

--

- On the other hand, inference for a difference of two means compares the same measurement on two difference populations. For example, we may want to study any difference between how caffeine affects the sleep of individuals with high blood pressure compared to those that do not have high blood pressure.   

--

- We can still use a t-distribution for inference for a difference of two means but we must compute the two-sample means and standard deviations separately for estimating standard error. 

--


We proceed with the details. 

---

# Confidence Intervals for a Difference in Means

- The t-distribution can be used for inference when working with the standardized difference of two means if

--

  - The data are independent within and between the two groups, *e.g.*, the data come from independent random samples or from a randomized experiment. 
  
--

  - We check the outliers rules of thumb for each group separately. 
  
--

- The standard error may be computed as

$$SE = \sqrt{\frac{\sigma_{1}^{2}}{n_{1}} + \frac{\sigma_{2}^{2}}{n_{2}}}$$

---

# Hypothesis Tests for Difference of Means

- Hypothesis tests for a difference of two means works in a very similar fashion to what we have seen before. 

--

- To conduct a test for a difference of two means "by hand", use the smaller of the two degrees of freedom.

--

- Use the `t.test` function without the `paired = TRUE` argument. 

---

# Examples of Inference for Difference of Means

- Let's look at some examples and work some problems together. 

---

# Power Calculations

- Often times in experiment planning, there are two competing considerations:

--

  - We want to collect enough data that we can detect important effects. 
  
--

  - Collecting data can be expensive, and in experiments involving people or animals, there may be some risk involved to the subjects.  

--

- The point is, from the beginning, we need to analyze what is an appropriate sample size for an experiment design and statistical tests. Power calculations is a methodology for doing so. 

---

# The Notion of Power

- Recall that a Type 1 Error occurs when we reject the null hypothesis when it is true. The significance level $\alpha$ is the probability of making a Type 1 Error. Thus, setting the significance level allows us to control the probability of making a Type 1 Error.

--

- Recall that a Type 2 Error occurs when we fail to reject the null hypothesis when the alternative hypothesis is true. How can we control Type 2 Error? 

--

- **Power** is the probability that we reject a null hypothesis when the corresponding alternative hypothesis is true. 

--

- In general, power is 1 minus the probability of making a Type 2 Error. Therefore, the larger the power of a test, the smaller the probability of making a Type 2 Error.

--

- Let's look at an example. 

---

# Power Example

- Suppose the heights of 2-year-old girls are normally distributed with a mean of 30 in. and a standard deviation of 6 in. Let $\mu$ denote the true mean heights of these girls. A researcher wishes to test $H_{0}:\mu=30$ vs. $H_{A}: \mu < 30$. 
--

- To test the hypothesis, the researcher plans to obtain a random sample of 30 2-year-old girls, measure their heights, and test with $\alpha = 0.05$. What is the probability of correctly rejecting the null hypothesis if the true mean is $\mu = 27$?

--

- First note that we will reject the null hypothesis if we obtain a Z-score that is less than or equal to 

```{r}
(z_crit <- qnorm(0.05)) 
```

--

- So, to compute the probability of correctly rejecting the null hypothesis to that is, the power, we need to compute the probability of getting a Z-score less than or equal to -1.645 whenever $\mu = 27$.

---

# Power Example Continued

- In our example we have 

```{r}
n <- 30
SE <- 6/sqrt(n)
mu_true <- 27
```

--

- Now note that the critical Z-score corresponds to an average height of

```{r}
(height_crit <- 30 + z_crit*SE)
```

--

- Thus, the power is the probability of observing an average height less than or equal to 28.198 whenever $\mu=27$. This is obtained as follows:

```{r}
z_for_pwr <- (height_crit - mu_true)/SE
(test_power <- pnorm(z_for_pwr))
```

Thus, we have about an 86% chance of correctly rejecting the null hypothesis if $\mu = 27$.


---

# Power and Sample or Effect Size

- In practice, we are often interested in computing the power for either different sample sizes, or different effect sizes. 

--

- The difference between the hypothesized mean $\mu_{0}$ and the true mean $\mu_{1}$ is an example of the notion of effect size. The larger the effect size, the more likely we would detect the difference. 

--

- If time permits, we will return later to a further discussion of power. The video included on the next slide is recommended for more on power. 

---

# Power Video

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/vnjjhQDedvs") %>%
  vembedr::use_align("center")
```



---

# Linear Regression

- Our next topic in the course is [linear regression](https://en.wikipedia.org/wiki/Linear_regression).

--

- Linear regression is one a fundamental method for statistical modeling. Much of advanced statistics builds on linear regression. So, it is important to become proficient in the practice of linear regression. 

--

- It is also important to have some understanding of what is going on "under the hood" in the construction and analysis of linear models. This is taken up in Chapter 8 of the textbook. To get started, please view the video on the next slide. 


---

# Intro to Regression Video 


```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/mPvtZhdPBhQ") %>%
  vembedr::use_align("center")
```