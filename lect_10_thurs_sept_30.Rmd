---
title: "Lecture 10"
subtitle: " "
author: "JMG"
institute: "MATH 204"
date: "Thursday, September 30"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
library(broom)
```


# Linear Regression: Introduction 

- Linear regression is the statistical method for fitting a line to data. 

--

- Recall that a (non-vertical) line in the $x,y$-plane is determined by 

$$y = \text{slope} \times x + \text{intercept}$$

--

- There are two aspects to fitting a line to data;

--

  - Estimating the slope and intercept values, and
  
--

  - Assessing the uncertainty of our estimates for the slope and intercept values.
 
--

- In this lecture, we cover all of the concepts necessary to understand how to carry out and interpret linear regression. 
  
--

- We encourage you to watch the video on the next slide to help in getting introduced to linear regression. 

---

# Regression Intro Video 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/mPvtZhdPBhQ") %>%
  vembedr::use_align("center")
```

---

# Learning Objectives

- After this lecture, you should

--

  - Understand the basic principles of simple linear regression: parameter estimates, residuals, and correlation.
  
--

  - Know the conditions for least squares regression: linearity, normality, constant variance, and independence.
  
--

  - Know how to obtain a linear fit using  R with the `lm` function. 
  
--

  - Be able to assess and interpret the results of a linear fit using  R with the `lm` function.
  
---

# Simple Regression Model

- Simple linear regression models the relationship between two (numerical) variables $x$ and $y$ by the formula

$$y = \beta_{0} + \beta_{1} x + \epsilon$$
where

--

  - $\beta_{0}$ (intercept) and $\beta_{1}$ (slope) are the model **parameters**

--

  - $\epsilon$ is the **error** 
  
--

- The parameters are estimated using data and their point estimates are denoted by $b_{0}$ (intercept estimate) and $b_{1}$ (slope estimate). 

--

- In linear regression, $x$ is called the explanatory or predictor variable while $y$ is called the response variable. 

--

- Let's look at an example data set for which linear regression is a potentially useful model. 

---

# Australian Brushtail Possum 


.center[
```{r possum, echo=FALSE, out.width="50%"}
knitr::include_graphics("https://www.dropbox.com/s/5hxo31udpanhnle/opossum.jpg?raw=1")
```
]

--

- Consider the `possum` data set, the first few rows of which are shown below

```{r}
head(possum,5)
```

---

# Possum Data Example

- Suppose that we as researchers are interested to study the relationship between the head length (`head_l`) and total length (`total_l`) of the brushtail possum of Australia. 

---

# Possum Regression 1

```{r ,echo=FALSE,fig.height=6,warning=FALSE,message=FALSE}
reg_dat <- possum
reg_dat %>% ggplot(aes(x=total_l,y=head_l)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_point(data=NULL,aes(x=92.0,y=89.5),color="red",size=3) +
  geom_segment(aes(x = 92.0, y = 89.5, xend = 92.0, yend = (42.7098+0.5729*92.0)), 
               data = NULL,
               color="purple",linetype="dashed") + 
  annotate("text", x = 94, y = 91.8, label = "A residual",size=5,color="purple")
```

---

# Possum Regression 2

```{r ,echo=FALSE,fig.height=6,warning=FALSE,message=FALSE}
lm_fit <- lm(head_l~total_l,data=reg_dat)

reg_dat_lm <- augment(lm_fit,reg_dat)
reg_dat_lm %>% ggplot(aes(x=total_l,y=head_l)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_segment(aes(x=total_l,y=head_l,xend=total_l,yend=.fitted),color="purple",linetype="dashed")

```

---

# Possum Regression 3

```{r ,echo=FALSE,fig.height=6,warning=FALSE,message=FALSE}
reg_dat_lm %>% ggplot(aes(x=total_l,y=.resid)) + geom_point() + 
  geom_hline(yintercept=0.0,linetype="dashed",color="blue")
```

---

# Fits and Residual Plots

.center[
```{r fits_resids, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/1q360mf7pgyn7t7/sampleLinesAndResPlots.png?raw=1")
```
]



---

# Correlation Illustrations


.center[
```{r corrs, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/jbgilwujdebjwwf/posNegCorPlots.png?raw=1")
```
]



---

# Regression Assumption Failures


.center[
```{r reg_assumps, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/hu01jaaqx2ohbgq/whatCanGoWrongWithLinearModel.png?raw=1")
```
]

