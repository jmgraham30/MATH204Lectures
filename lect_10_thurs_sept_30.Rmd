---
title: "Lecture 10"
subtitle: " "
author: "JMG"
institute: "MATH 204"
date: "Thursday, September 30"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(faraway)
library(tidyverse)
library(ggformula)
library(ggmosaic)
library(patchwork)
library(latex2exp)
library(kableExtra)
library(broom)
```


# Linear Regression: Introduction 

- Linear regression is a statistical method for fitting a line to data. 

--

- Recall that a (non-vertical) line in the $x,y$-plane is determined by 

$$y = \text{slope} \times x + \text{intercept}$$

--

- There are two aspects to fitting a line to data that we will study:

--

  - Estimating the slope and intercept values, and
  
--

  - Assessing the uncertainty of our estimates for the slope and intercept values.
 
--

- In this lecture, we cover all of the concepts necessary to understand how to carry out and interpret linear regression. 
  
--

- We encourage you to watch the video on the next slide to help in getting introduced to linear regression. 

---

# Regression Intro Video 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/mPvtZhdPBhQ") %>%
  vembedr::use_align("center")
```

---

# Learning Objectives

- After this lecture, you should

--

  - Understand the basic principles of simple linear regression: parameter estimates, residuals, and correlation.
  
--

  - Know the conditions for least squares regression: linearity, normality, constant variance, and independence.
  
--

  - Know how to obtain a linear fit using  R with the `lm` function. 
  
--

  - Be able to assess and interpret the results of a linear fit using  R with the `lm` function.
  
---

# Simple Regression Model

- Simple linear regression models the relationship between two (numerical) variables $x$ and $y$ by the formula

$$y = \beta_{0} + \beta_{1} x + \epsilon$$
where

--

  - $\beta_{0}$ (intercept) and $\beta_{1}$ (slope) are the model **parameters**

--

  - $\epsilon$ is the **error** 
  
--

- The parameters are estimated using data and their point estimates are denoted by $b_{0}$ (intercept estimate) and $b_{1}$ (slope estimate). 

--

- In linear regression, $x$ is called the explanatory or predictor variable while $y$ is called the response variable. 

--

- Let's look at an example data set for which linear regression is a potentially useful model. 

---

# Australian Brushtail Possum 

.center[
```{r possum, echo=FALSE, out.width="45%"}
knitr::include_graphics("https://www.dropbox.com/s/5hxo31udpanhnle/opossum.jpg?raw=1")
```
]

--

- The `possum` data set records measurements of 104 brushtail possums from Australia and New Guinea, the first few rows of the data are shown below

```{r}
head(possum,4)
```

---

# Possum Data Example

- Suppose that we as researchers are interested to study the relationship between the head length (`head_l`) and total length (`total_l`) measurements of the brushtail possum of Australia. 

--

- Note that head length (`head_l`) and total length (`total_l`)  are both (continuous) numerical variables.  

--

- The next slide displays a scatterplot for `head_l` versus `total_l`.

---

# Possum Data Scatterplot

```{r ,echo=FALSE,fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
reg_dat <- possum
reg_dat %>% ggplot(aes(x=total_l,y=head_l)) + 
  geom_point(size=3) + theme(axis.text= element_text(face="bold", 
                           size=14),
  axis.title=element_text(size=18,face="bold"))
```

--

- Describe the features of any association that there appears to be between the two variables in the plot. 

---

# Possum Data Regression Line

- We have added the "best fit" line to the scatter plot of `head_l` versus `total_l`. Later we discuss how this line is obtained. 

```{r ,echo=FALSE,fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
reg_dat %>% ggplot(aes(x=total_l,y=head_l)) + 
  geom_point(size=3) + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_point(data=NULL,aes(x=92.0,y=89.5),color="red",size=5) +
  geom_segment(aes(x = 92.0, y = 89.5, xend = 92.0, yend = (42.7098+0.5729*92.0)), 
               data = NULL,
               color="purple",linetype="dashed",lwd=1) + 
  annotate("text", x = 94, y = 91.8, label = "A residual",size=7,color="purple") + theme(axis.text= element_text(face="bold", 
                           size=14),
  axis.title=element_text(size=18,face="bold"))
```

--

- A **residual** is the vertical distance between a data point and the best fit line.  The next slide shows all of the residuals for the data. 

---

# Possum Regression Residuals

```{r ,echo=FALSE,fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
lm_fit <- lm(head_l~total_l,data=reg_dat)

reg_dat_lm <- augment(lm_fit,reg_dat)
reg_dat_lm %>% ggplot(aes(x=total_l,y=head_l)) + 
  geom_point(size=3) + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_segment(aes(x=total_l,y=head_l,xend=total_l,yend=.fitted),color="purple",linetype="dashed") + theme(axis.text= element_text(face="bold", 
                           size=14),
  axis.title=element_text(size=18,face="bold"))

```

--

- The best fit or regression line is the line that minimizes all of the residuals simultaneously.  

---

# Possum Regression Residual Plot

- A residual plot displays the residual values versus the $x$ values from the data.

```{r ,echo=FALSE,fig.height=5,fig.width=10,warning=FALSE,message=FALSE}
reg_dat_lm %>% ggplot(aes(x=total_l,y=.resid)) + geom_point(size=3) + 
  geom_hline(yintercept=0.0,linetype="dashed",color="blue",lwd=1) + theme(axis.text= element_text(face="bold", 
                           size=14),
  axis.title=element_text(size=18,face="bold"))
```

--

- As we will see, residual plots play an important role in assessing the results of a regression model. 

---

# Fits and Residual Plots

- Let's look some linear fits and their corresponding residual plots. 

--

.center[
```{r fits_resids, echo=FALSE, out.width="80%"}
knitr::include_graphics("https://www.dropbox.com/s/1q360mf7pgyn7t7/sampleLinesAndResPlots.png?raw=1")
```
]

--

- In the first column, the residuals show no obvious pattern, this is desirable. In the second column, the residuals show a pattern that suggests a linear model is inappropriate. In the third column, it's not clear if the linear fit is statistically significant. 

---

# Correlation

- **Correlation**, which always takes values between -1 and 1 is a statistic that describes the strength of the linear relationship between two variables. Correlation is denoted by $R$. 

--

- In R, correlation is computed with the `cor` command. For example, the correlation between the `head_l` and `total_l` variables in the `possum` data set is computed as

```{r}
cor(possum$head_l,possum$total_l)
```

--

- The plot in the next slide shows several scatter plots together with the corresponding correlation value.  

---

# Correlation Illustrations


.center[
```{r corrs, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/jbgilwujdebjwwf/posNegCorPlots.png?raw=1")
```
]

---

# Strongly Related Variables with Weak Correlations

- It is important to note that two variables may have a strong association even if their correlation is relatively weak. This is because correlation measures **linear** association and variables may be a strong **nonlinear** association. 

--

.center[
```{r nonlin_cor, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/rmujuxuoihdg5xb/corForNonLinearPlots.png?raw=1")
```
]


---

# Least Squares Regression

- We now begin to discuss the details of how to fit a simple linear regression model to data. 

--

- The approach we take is called *least squares regression*. 

--

- The idea is to chose parameter estimates that minimize all of the residuals simultaneously. That is, for each observed data point $(x_{i},y_{i})$, we find $b_{0}$ and $b_{1}$ such that if $\hat{y}_{i} = b_{0} + b_{1}x_{i}$, then

$$RSS = \sum_{i=1}^{n}(\hat{y}_{i} - y_{i})^2$$

is as small as possible. 

--

- For this to work out well, several conditions need to be met. These conditions are spelled out on the next slide. 

---

# Conditions for Least Squares

- **Linearity.** The data should show a linear trend. 

--

- **Normality.** Generally, the distribution of the residuals should be close to normal. 

--

- **Constant Variance.** The variability of points around the least squares line remains roughly constant. Residual plots are a good way to check this condition. 

--

- **Independence.** We want to avoid fitting a line to data via least squares whenever there is dependence between consecutive data points. 

--

- The next slide shows plot of data where at least one of the conditions for least squares regression fails to hold. 

---

# Regression Assumption Failures


.center[
```{r reg_assumps, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/hu01jaaqx2ohbgq/whatCanGoWrongWithLinearModel.png?raw=1")
```
]

--

- In the first column, the linearity condition fails. In the second column, the normality condition fails. 

--

- In the third column, the constant variance condition fails. In the fourth column, the independence condition fails. 

--

- Notice how in each case, the residual plot can be used to diagnose problems with a least squares linear regression fit. 


---

# Fitting a Linear Model

- For simple least squares linear regression with one predictor variable ( $x$ ), one can fit the model to data "by hand". The mathematical formula is

$$b_{1} = \frac{s_{y}}{s_{x}} R,$$

$$b_{0} = \bar{y} - b_{1} \bar{x},$$

where

  - $R$ is the correlation between $x$ and $y$,

--
  
  - $s_{y}$ and $s_{x}$ are the sample standard deviations for $y$ and $x$, and 
  
--

  - $\bar{y}$ and $\bar{x}$ are the sample means for $y$ and $x$. 
  
--

- Let's apply these formulas to the `possum` data set with $x$ the `total_l` variable and $y$ the `head_l` variable. 

---

# Applying the Regression Formulas

- We need to compute the correlation, sample means, and sample standard deviations:

```{r}
x <- possum$total_l; y <- possum$head_l
x_bar <- mean(x); y_bar <- mean(y)
s_x <- sd(x); s_y <- sd(y)
R <- cor(x,y)
```

--

- Now we can compute our estimates $b_{0}$ and $b_{1}$:

```{r}
(b_1 <- (s_y/s_x)*R)
(b_0 <- y_bar - b_1*x_bar)
```

--

- There is an R command, `lm` (linear model) that will compute these values and much more for us. 

---

# The lm Command

- Let's see an example of how the `lm` command is used:

```{r}
lm(head_l ~ total_l, data=possum)
```

--

- Notice that this returns the point estimate values for $b_{0}$ (Intercept) and the slope $b_{1}$, and that these values are the same as what we obtained using the mathematical formulas on the last slide. 

---

# Interpreting Model Parameters

- For a linear model,

--

  - The slope describes the estimate difference in the $y$ variable if the explanatory variable $x$ for a case happened to be one unit larger. 
  
--

 - The intercept describes the average outcome of $y$ if $x=0$ **and** the linear model is valid all the way to $x=0$, which in many applications is not the case.
 
--

- To evaluate the strength of a linear fit, we compute $R^{2}$ (R-squared). The value of $R^{2}$ tells us the percent of variation in the response that is explained by the explanatory variable.  

--

- There are some pitfalls in interpreting the results of a linear model. In particular, 

--

  - Applying a model to estimate values outside of the realm of the original data is called **extrapolation**. Generally, extrapolation is unreliable. 
  
--

  - In many cases, even when there is a real association between variables, we cannot interpret a causal connection between the variables. 
  
---

# Example

- Before we discuss further details of regression, let's look at a detailed example of fitting and interpreting a linear model.  

--

- We will look at a linear model for the data set, `cheddar` from the `faraway` package.

--

- Let's do this example together in RStudio. 
  

---

# Outlier Issues


---

# Inference for Regression


---

# More R For Linear Models



