---
title: "Lecture 8"
subtitle: "Foundations for Inference"
author: "JMG"
institute: "MATH 204"
date: "Thursday, September 23"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(tidyverse)
library(ggformula)
library(patchwork)
library(latex2exp)
library(kableExtra)
```

# Statistical Inference

- The goal of statistical inference it to use sample data to infer some information about a population. 

--

- There is a particular flow of logic that we will follow in this course as we conduct various specific statistical analyses. 

--

- The goal of this lecture is to explain this flow of logic and to define the important terms and concepts that we use to talk about statistical inference. 

--

- The main ideas are 

--

  - parameter estimates, or point estimates
  
--

  - confidence intervals
  
--

  - tests of statistical hypotheses
  
--

- Note that all of these concepts are covered in Chapter 5 of the textbook. The videos inlcuded in the next few slides are recommended. 


  
---

# Central Limit Theorem Video 

- You are encouraged to watch this video on point estimates and the central limit theorem. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/oLW_uzkPZGA") %>%
  vembedr::use_align("center")
```

---

# Confidence Intevals Video

- You are encouraged to watch this video on confidence intervals. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/A6_W8qY8zJo") %>%
  vembedr::use_align("center")
```

---

# Hypothesis Testing Video 

- You are encouraged to watch this video on hypothesis testing. 

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/NVbPE1_Cbx8") %>%
  vembedr::use_align("center")
```


---

# Some Motivation

- Random variables and probability distributions provide models for random processes and help us to compute the probability of outcomes for a random process. 
--

- For example, if we toss a coin 100 times and we know that the coin is fair, then we can model this process by a binomial random variable and use a binomial distribution with $n=100$ and $p=\frac{1}{2}$ to compute the probability of getting some number of heads, say $k$, out of 100 tosses. 

--

- There is a kind of inverse question though. Given a coin or type of coin, how do we know if it is a fair coin or not. It seems reasonable to assume that the number of heads $k$ out of $n$ tosses follows a binomial distribution for some value $p$ for the probability of getting heads, but what specific value of $p$? 

--

- What we can do is to take a sample, that is, to flip the coin some number of times and then take the proportion $\hat{p}$ of the heads that appear out of the total number of tosses. Then, we expect $\hat{p}$ to be a good indicator of the value of $p$. In other words, we use sample data and a sample statistic to estimate the value of an unknown population parameter. 

---

# Estimation and Hypothesis Testing

- So, we use data to estimate parameter values. 

--

- However, each time we repeat our estimation procedure we are likely to obtain a different estimate $\hat{p}$ due to natural variability. Thus our estimate $\hat{p}$ is really a random variable called the **sample proportion**. 

--

- It's not enough to have just an estimate, we also need to know or estimate how much variability to reasonably expect for our estimate $\hat{p}$ as a random variable. 

--

- Furthermore, one can ask a related but slightly different question. Maybe we don't want an accurate estimate for the probability $p$ of getting heads, maybe all we want to know is if the coin is fair ( $p = \frac{1}{2}$ ) or not ( $p \neq  \frac{1}{2}$ ). Addressing this type of question is known as hypothesis testing. 

--

- In statistical hypothesis testing, we state two mutually exclusive hypotheses (*e.g.*, the coin is fair or it is not). Then we collect evidence in the form of sample data. Finally, we ask how likely is it to observe the sample data if we assume that  one of the specific mutually exclusive hypotheses is in fact true. 


---

# Next Steps

- In order to get a feeling for statistical inference, we look at a particular example situation that we hope is fairly intuitive to understand. 

--

- In the context of our example, we study

--

  - point estimation, then
  
--

 - confidence intervals, and finally
 
--

 - hypothesis testing.
 
--

- We will start with a computational example to get a feel for what is going on.


---

# A Computational Example

- Suppose we want to estimate the probability of getting heads (success) $p$ for a coin. In order to do so, we start by tossing the coin 100 times, adding up the number of heads, and dividing the total number of heads by 100. Then we get an observed sample proportion:

$$\hat{p} = \frac{\text{num. heads after 100 tosses}}{100}$$

- For example, suppose we get $\hat{p} = \frac{47}{100}=0.47$. 

--

- Let's repeat this process a large number of times and record the outcomes. The first few rows of our data looks as follows.

```{r samp_dat, echo=FALSE}
get_ps <- function(i){
  n <- 100
  samp <- sum(rbinom(n,1,0.5))
  p_hat <- samp / n
  return(p_hat)
}

N <- 10000
dts <- map_dbl(1:N,get_ps)

p_df <- tibble(p_estimate=dts)
p_df %>% head()
```


---

# Histogram of Data for Example

- Let's look at a histogram of our data. 

```{r p_hist,echo=FALSE,fig.height=4}
p_df %>% gf_histogram(~p_estimate,color="black",binwidth=0.025)
```

--

- This histogram provides insight into what the distribution of the sample proportion $\hat{p}$ might be. We refer to this distribution as **the sampling distribution of the sample proportion**. It turns out to be very close to normal, but with what $\mu$ and $\sigma$?

---

# The Central Limit Theorem for Proportion

- The **central limit theorem** for the sample proportion says the following:

> For a sample of size $n$, the sampling distribution for the sample proportion is very close to $N(\mu=p,\sigma=\sqrt{\frac{p(1-p)}{n}})$, where $p$ is the true value of the population proportion. This is provided the sample size $n$ is sufficiently large. 

--

- In lecture 7, we saw that the sampling distribution of the sample mean is also very close to normal. Thus, there is also a central limit theorem for the sample mean and we will discuss this more later in the course. 

--

- The point here is that, when the central limit theorem applies, we can use a normal distribution to assess the uncertainty in our point estimates. For example, to estimate **standard error** and obtain confidence intervals. 

--

- Furthermore, when the central limit theorem applies, we can use a normal distribution for conducting hypothesis tests. This will be discussed later.

--

- Before we go on, let's explore the concepts of standard error and confidence intervals. 

---

# Standard Error

- The standard deviation of a sampling distribution is called **standard error**. 

--

- For example, the standard deviation of the sample proportion $\hat{p}$ would be the standard error for $\hat{p}$. By the central limit theorem, the standard error for $\hat{p}$ is (very nearly)

$$SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}},$$
where $p$ is the true population proportion. 

--

- The central limit theorem is a powerful theoretical result. However, in practice we do not know the value for the true population proportion $p$. If we did, there would be no need for inference. 

--

- In practice, we use the "plug-in principle" to estimate $SE_{\hat{p}}$ by substituting our estimate $\hat{p}$ for the true population proportion to obtain an estimate

$$SE_{\hat{p}} \approx \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.$$

---

# Confidence Intervals for a Proportion

- The sample proportion $\hat{p}$ provides a plausible value for the population proportion $p$. However, there is some standard error associated with it. 

--

- When stating an estimate for the population proportion, it is better practice to provide a plausible *range of values* instead of just a point estimate alone. 

--

- The standard error represents the standard deviation of the point estimate, and when the central limit theorem applies, the point estimate closely follows a normal distribution. In a normal distribution, 95% of the data is within 1.96 standard deviations of the mean. 

--

- In the case when the central limit theorem applies, we call the formula

$$\text{point estimate} \pm 1.96 \times SE$$
a **95% confidence interval**. 

--

- We will work out some examples together soon. Before doing so, let's try to understand the meaning of 95%. 


---

# Meaning of 95 Percent CI

```{r ninty_five,echo=FALSE,fig.width=10}
library(tidyverse)

n <- 100
tp <- 0.5

mu_p <- tp
SE_p <- sqrt((tp*(1-tp))/n)

get_est <- function(i){
  samp <- rbinom(n,1,tp)
  p_hat <- sum(samp) / n
  lwr <- p_hat - 1.96 * SE_p
  uppr <- p_hat + 1.96 * SE_p
  dt <- list(index=i,estimate=p_hat,a=lwr,b=uppr)
  return(dt)
}

N <- 100

CI_list <- map(1:N,get_est)

df_list <- function(x_list){
  x_df <- tibble(index=x_list$index,estimate=x_list$estimate,a=x_list$a,b=x_list$b)
  return(x_df)
}

CI_df <- map_df(CI_list,df_list)

CI_df %>% 
  ggplot(aes(x=index,y=estimate)) + 
  geom_point(color="#88CCEE",size=2) + 
  geom_hline(yintercept = tp,linetype="dashed") + 
  geom_linerange(aes(ymin=a,ymax=b),color="#CC6677") + ylim(c(0,1))
```

---

# Break for Examples

- Let's take a break from the slides and work some examples for the concepts that we have covered so far. 

---

# Hypothesis Testing



