---
title: "Lecture 7"
subtitle: "The Normal Distribution"
author: "JMG"
institute: "MATH 204"
date: "Tuesday, September 21"
output:
  xaringan::moon_reader:
    css: [rladies, rladies-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(openintro)
library(tidyverse)
library(ggformula)
library(patchwork)
library(latex2exp)
library(kableExtra)
```


# Learning Objectives

In this lecture, we will

- Study the **normal distribution** and learn how to compute probability values for a random variable that follows a normal distribution.

--

- The normal distribution is a continuous distribution. We will discuss the so-called normal curve which is the probability density function for a normal distribution. 

--

- We will also learn about the expected value and variance for a normal random variable. 

--

- This lecture corresponds to section 4.1 in the textbook, and you are encouraged to watch the lecture video included in the next slide. 


---

# Normal Distribution Video

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/S_p5D-YXLS4") %>%
  vembedr::use_align("center")
```

---

# Samples from a Normal Distribution

- The following histogram shows 5,000 random samples from a normal random variable:

```{r norm_samps,fig.height=5,fig.width=8}
tibble(x=rnorm(5000,66,3.5)) %>% gf_histogram(~x,color="black")
```

---

# Normal Histogram Discussion 

- Think of the histogram on the previous slide as showing sample data for measurements of human heights in inches.  

--

- What are the key features of the histogram?

--

- It is unimodal, highly symmetric, and centered at the mean.

--

- Do you think that such data could reasonably correspond to measurements of human heights in inches?

--

- Do you think it is reasonable to treat measurements of human heights in inches as a continuous variable? 

---

# Normal Distribution Facts

> Many random variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems. We will use it in data exploration and to solve important problems in statistics. 

--

- Let's spend some time to develop some intuition for how the normal distribution is often used in practice.

--

- We start by generating some data that is not necessarily normally distributed. 

---

# Sampling a Mean

- We are going to play a game. We proceed as follows: 

--

  1) Sample 15 values from a binomial random variable with $n=25$ and $p=0.5$. You can think of this as doing 15 rounds of an experiment where each time we flip a fair coin 25 times and count the number of heads that we obtain.
  
--

  2) We compute and record the mean of the 15 values obtained in step 1. 

--

  3) We repeat steps 1 & 2 a very large number of times, say 2,500. 
  
--

- Here is the first few rows of a data frame that contains the data we acquire by playing our game.

```{r samp_mean,echo=FALSE}
get_means <- function(i){
  return(mean(rbinom(15,25,0.5)))
}

ms <- map_dbl(1:2500,get_means)
means_df <- tibble(means=ms)
means_df %>% head()
```

---

# Plotting Our Data

- Here is a histogram of the data we obtained from the game described on the previous slide:

```{r samp_mean_hist,echo=FALSE,fig.height=4,fig.width=8}
means_df %>% gf_histogram(~means,color="black")
```

--

- The mean of our data is `r round(mean(ms),3)`. What is the expected value of a binomial random variable with $n=25$ and $p=0.5$? 

---

# A Normal Density Function

- The following plot shows a **normal** density curve:

```{r norm_dens,fig.height=5}
gf_dist("norm")
```

---

# Center and Shape of a Normal Curve

- There are two **parameters** called $\mu$ (mean) and $\sigma$ (sd) that determine the center and shape, respectively of a normal curve. For example,:

```{r norm_comps,echo=FALSE,fig.height=5,fig.width=8,warning=FALSE}
lab1a <- TeX("$\\mu = 0.0$")
lab1b <- TeX("$\\sigma = 1.0$")
lab2a <- TeX("$\\mu = 1.0$")
lab2b <- TeX("$\\sigma = 0.5$")

ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), col='#88CCEE',lwd=1) +
  stat_function(fun = dnorm, args = list(mean = 1, sd = .5), col='#CC6677',lwd=1) + geom_vline(xintercept = 0,linetype="dashed",col="#88CCEE") + 
  geom_vline(xintercept = 1,linetype="dashed",col="#CC6677") + 
  annotate(geom="text",x=-1.5,y=0.3,label=lab1a,col="#88CCEE",size=6) + 
  annotate(geom="text",x=-2,y=0.2,label=lab1b,col="#88CCEE",size=6) + 
  annotate(geom="text",x=2,y=0.7,label=lab2a,col="#CC6677",size=6) + 
  annotate(geom="text",x=2,y=0.6,label=lab2b,col="#CC6677",size=6)
  
```

---

# Going Back to Data

- Recall that to obtain our data, we drew 15 **independent** samples from a binomial distribution with $n=25$ and $p=0.5$, took the sample mean, and repeated this many times. The following plot shows the histogram of our data (with density instead of count on the $y$-axis) and overlays a particular normal density curve. 

```{r norm_fit,echo=FALSE,fig.height=5,fig.width=8,warning=FALSE}
n <- 25
p <- 0.5
labSa <- TeX("$\\mu = np$")
labSb <- TeX("$\\sigma = \\frac{\\sqrt{np(1-p)}}{\\sqrt{n}}$")
means_df %>% ggplot(aes(x = means)) + 
    geom_histogram(aes(y =..density..),binwidth = 0.25,
                   colour = "black",fill="grey") +
stat_function(fun = dnorm, args = list(mean = n*p, sd = sqrt(p*(1-p))),color="#CC6677",lwd=1) + 
  annotate(geom="text",x=13,y=0.8,label=labSa,col="#CC6677",size=6) + 
  annotate(geom="text",x=13.5,y=0.7,label=labSb,col="#CC6677",size=6)

```

---

# Sampling Distribution of the Mean

- We can think of the sample mean as a random variable. The sample mean inputs 
sample data of a fixed size from a population and returns the mean of the data. 

--

- The point is that the sample mean will vary as the sample varies. 

--

- What the last slide shows us is that, at least in the particular example, the distribution of the sample mean (viewed as a random variable) is very close to a random variable that is normally distributed.

--

- In general, we call the distribution of the sample mean (viewed as a random variable), the **sampling distribution of the mean**.  It is a general fact that the sampling distribution of the mean (for independent samples) is always very close to a normal distribution, regardless of the type of distribution used to sample the data for which the sample mean is computed. 

--

- This is the reason why the normal distribution plays such a central role in statistics. 

---

# Notation for Normal Random Variables

- If $X$ is a normal random variable with expected value $\mu=E(X)$ and standard deviation $\sigma = \sqrt{\text{Var}(X)}$, then we write $X \sim N(\mu,\sigma)$. 

--

- For example, if we have a normal random variable $X$ with expected value $12.5$ and standard deviation $0.5$, then we write $X \sim N(\mu=12.5,\sigma=0.5)$. 

--

- We call a random variable $Z$ that satisfies $Z \sim N(\mu=0,\sigma=1)$ a **standard normal variable** and we call the normal distribution with $\mu=0$ and $\sigma=1$ the **standard normal distribution**. 

--

- Our next goal is to see how to use the normal density function to compute probability values for a random variable that follows a normal distribution. 

---

# Standardizing with Z-scores

- Let $X \sim N(\mu,\sigma)$ be a normal random variable. 

--

- Define a new random variable $Z = \frac{X - \mu}{\sigma}$. We can compute the expectation and variance of $Z$ as follows:

--

$$E\left(Z\right)=E\left(\frac{X - \mu}{\sigma}\right)=\frac{1}{\sigma}E\left(X - \mu\right) = \frac{1}{\sigma}(E(X) - \mu) = \frac{1}{\sigma}(\mu - \mu) = 0$$

--

$$\text{Var}\left(Z\right)=\text{Var}\left(\frac{X - \mu}{\sigma}\right)=\frac{1}{\sigma^2}\text{Var}\left(X - \mu\right)=\frac{1}{\sigma^2}\text{Var}(X) = \frac{\sigma^2}{\sigma^2}=1$$

--

- So, if $X\sim N(\mu,\sigma)$, then $Z\sim N(0,1)$, where $Z = \frac{X-\mu}{\sigma}$. 


--

- This is all a little abstract, so let's think about standardizing from a different perspective. 

---

# Standardizing Data

- The following table shows the mean and standard deviation for total scores on the SAT and ACT exams:

```{r exam_stats,echo=FALSE}
exam_stats <- data.frame(SAT=c(1100,200),ACT=c(21,6),row.names = c("Mean","SD"))
exam_stats %>% kbl() %>% kable_styling()
```

--

- Suppose we know that one person scored 1300 on the SAT and another person scored 24 on the ACT. Who had the better exam score? The numbers alone can not be compared. But their standardized $Z$-scores can.

--

$$\text{person 1 Z-score}=\frac{1300-1100}{200}=1,\ \ \text{person 2 Z-score}=\frac{24-21}{6}=\frac{1}{2}$$

--

- So we conclude that person 1 had the better exam score. 

--

- Note that observations above the mean have a positive $Z$-score while observations below the mean have a negative $Z$-score. 

---

# Probability for Normal R.V.'s

- The first think to know is that the total area under a normal density function is equal to 1. This is true regardless of the values for $\mu$ and $\sigma$. Thus, the shaded area shown below is 1:

```{r unit_area,echo=FALSE,fig.height=5}
ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + 
  annotate(geom="text",x=0,y=0.2,label="total area is 1",size=6)
```

---

# Tail Areas


- Suppose that $X \sim N(\mu,\sigma)$, how do we find an area such as one of the ones shown below. That is, how do we find the area under one of the tails of a normal density function? 

```{r tail_areas,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_l <- function(x) {
    y <- dnorm(x)
    y[x >= -1.5] <- NA
    return(y)
}

dnorm_limit_r <- function(x) {
    y <- dnorm(x)
    y[x < 1.8] <- NA
    return(y)
}

p_1 <- ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_l, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = -1.5,linetype="dashed",color="red") + 
  annotate(geom="text",x=-2.1,y=0.3,label="x=-1.5",size=6,color="red") + labs(caption="A left tail area") + theme(plot.caption=element_text(hjust=0,size=18))

p_2 <- ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_r, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = 1.8,linetype="dashed",color="red") + 
  annotate(geom="text",x=2.3,y=0.3,label="x=1.8",size=6,color="red") + labs(caption="A right tail area") + theme(plot.caption=element_text(size=18))

p_1 + p_2
```

---

# Interpreting Tail Areas

- In the last slide, the shaded area on the left represents the probability of an outcome being less than or equal to -1.5. 

--

- In the last slide, the shaded area on the right represents the probability of an outcome being greater than or equal to 1.8.

--

- We will use $Z$-scores and the standard normal distribution to compute such areas. 

--

- We proceed by first learning to compute tail areas under the standard normal density curve which corresponds to a random variable $Z \sim N(\mu=0,\sigma=1)$. 

---

# Standard Normal Tail Areas

- We spell out the steps for computing tail areas under the standard normal density function. 

--

  - First, draw a picture of the tail area you want to compute. 
  
--
  
  - Decide if it's a left tail area or a right tail area. 
  
--

  - If it's a left tail area, use `pnorm(z)` in R to compute the value.
  
--

 - If it's a right tail area, use `1 - pnorm(z)` in R to compute the value. 
 
--

- The next slide explains why this approach works. 

---

# The Meaning of `pnorm`

- The R command `pnorm(z)` computes the area under the standard normal density function for values less than or equal to $z$. That is, it computes the tail area to the left of the value `z`. 

```{r left_tail_area,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_l2 <- function(x) {
    y <- dnorm(x)
    y[x >= -0.5] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_l2, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = -0.5,linetype="dashed",color="red") + 
  annotate(geom="text",x=-0.6,y=0.38,label="z",size=6,color="red") +
  annotate(geom="text",x=-1.2,y=0.05,label="area = pnorm(z)",size=6) +
  labs(caption="A left tail area") + theme(plot.caption=element_text(hjust=0,size=18))

```


---

# Right Tail Areas

- This figure explains why we use `1-pnorm(z)` to compute right tail areas.

```{r right_tail_area,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_l3 <- function(x) {
    y <- dnorm(x)
    y[x >= 0.5] <- NA
    return(y)
}

dnorm_limit_r3 <- function(x) {
    y <- dnorm(x)
    y[x <= 0.5] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_l3, geom = "area", fill = "#CC6677") +
  stat_function(fun = dnorm_limit_r3, geom = "area", fill = "blue",alpha=0.5) +
  stat_function(fun = dnorm) + geom_vline(xintercept = 0.5,linetype="dashed",size=1) + 
  annotate(geom="text",x=0.65,y=0.38,label="z",size=6) +
  annotate(geom="text",x=-0.8,y=0.1,label="area = pnorm(z)",size=6) + 
  annotate(geom="text",x=1.3,y=0.04,label="area = 1-pnorm(z)",size=6)

```

--

- Since the total area is 1 and `pnorm(z)` gives the left tail area it must be that `1-pnorm(z)` gives the right tail area. 

---

# Left Tail Example

- What is the area under the standard normal density curve to the left of -2? That is, what is the area shown in the figure? 

```{r left_examp,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_l_examp <- function(x) {
    y <- dnorm(x)
    y[x >= -2] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_l_examp, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = -2,linetype="dashed",color="red") + 
  annotate(geom="text",x=-2.3,y=0.2,label="z=-2",size=6,color="red")

```


--

- It is `pnorm(-2) = ` `r pnorm(-2)`. 


---

# Right Tail Example

- What is the area under the standard normal density curve to the right of 1? That is, what is the area shown in the figure? 

```{r right_examp,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_r_examp <- function(x) {
    y <- dnorm(x)
    y[x <= 1] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_r_examp, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = 1,linetype="dashed",color="red") + 
  annotate(geom="text",x=1.1,y=0.38,label="z=1",size=6,color="red")

```


--

- It is `1-pnorm(1) = ` `r 1-pnorm(1)`. 


---

# Middle Areas

- You are probably curious as to how we can compute a "middle" area such as the one shown below.

```{r mid_area,echo=FALSE,fig.height=5,fig.width=10}
dnorm_limit_mid <- function(x) {
    y <- dnorm(x)
    y[x <= -0.5 | x >= 1.2] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_mid, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = -0.5,linetype="dashed",size=1) + geom_vline(xintercept = 1.2,linetype="dashed",size=1) +   annotate(geom="text",x=-0.8,y=0.0,label="z=-0.5",size=6) + 
annotate(geom="text",x=1.5,y=0.0,label="z=1.2",size=6) + 
  annotate(geom="text",x=0.3,y=0.15,label="area=?",size=6)

```


--

- The answer is simple, find the left **and** right tail areas and then subtract from 1. 

---

# Middle Area Example

- To compute the area shown:

```{r mid_area_examp,echo=FALSE,fig.height=4,fig.width=8}
dnorm_limit_mid <- function(x) {
    y <- dnorm(x)
    y[x <= -0.5 | x >= 1.2] <- NA
    return(y)
}

ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = dnorm_limit_mid, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm) + geom_vline(xintercept = -0.5,linetype="dashed",size=1) + geom_vline(xintercept = 1.2,linetype="dashed",size=1) +   annotate(geom="text",x=-0.8,y=0.0,label="z=-0.5",size=6) + 
annotate(geom="text",x=1.5,y=0.0,label="z=1.2",size=6) + 
  annotate(geom="text",x=0.3,y=0.15,label="area=?",size=6)

```


--

```{r}
l_area <- pnorm(-0.5) # compute left tail area
r_area <- 1 - pnorm(1.2) # copute right tail area
(mid_area <- 1 - (l_area + r_area)) # subtract tails areas from 1
```

---

# Alternative Method for Mid Areas

- Notice that the following computation also finds the mid area shown on the previous slide

```{r}
pnorm(1.2) - pnorm(-0.5)
```

--

- Take a minute to think about why this works and then we will discuss together. 

--

- What does `pnorm(1.2)` represent? Draw the area under the standard normal density curve that it represents. 

--

- What does `pnorm(-0.5)` represent? Draw the area under the standard normal density curve that it represents. 

--

- Now what does the difference `pnorm(1.2) - pnorm(-0.5)` represent? 

---

# Areas for General Normal Density Curves

- Suppose that $X \sim N(\mu = 10, \sigma=2.5)$. How can we find the left tail area the density curve corresponding to $N(\mu = 10, \sigma=2.5)$ that lies to the left of $x=7.3$? This area is shown below.

```{r gen_left_examp,echo=FALSE,fig.height=4,fig.width=8}
dnorm_gen_l_examp <- function(x) {
    y <- dnorm(x,mean=10,sd=2.5)
    y[x >= 7.3] <- NA
    return(y)
}

ggplot(data.frame(x = c(10-3*2.5, 10+3*2.5)), aes(x = x)) +
  stat_function(fun = dnorm_gen_l_examp, geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dnorm,args=list(mean=10,sd=2.5)) + geom_vline(xintercept = 7.3,linetype="dashed",color="red") + 
  annotate(geom="text",x=6.6,y=0.15,label="x=7.3",size=6,color="red")

```

--

- The answer is, we first standardize to find $z=\frac{x-\mu}{\sigma}=\frac{7.3-10}{2.5}=$ `r (7.3-10.0)/2.5`, then we compute `pnorm(z) = ` `r pnorm((7.3-10.0)/2.5)`. 

---

# General Steps for Normal Curve Areas

- To compute the area under a normal density curve for $N(\mu,\sigma)$, 

--

  - First draw a picture and determine if it is a left tail area, right tail area, or middle area. 
  
--

  - Then standardize by subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$. 
  
--

  - Finally, use `pnorm` in R as we have explained over the last several slides. 
  
--

- We take a break from the slides to work out some examples together. 

---

# The 68-95-99.7 Rule

- The following figure shows the percentage of area under a normal curve that lies within 1, 2, and 3 standard deviations of the mean, respectively.

.center[
```{r 6895997, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://www.dropbox.com/s/2e73g42uinqq5gz/6895997.png?raw=1")
```
]


---

# Worked Examples

- Let's take a break from lecture slides to go through some worked examples using the normal distribution. 

---

# Foundations for Inference

- We have now developed the probability tools we need to begin to discuss statistical inference. Before the next lecture, please watch this video:

```{r,echo=FALSE}
vembedr::embed_url("https://youtu.be/oLW_uzkPZGA") %>%
  vembedr::use_align("center")
```


